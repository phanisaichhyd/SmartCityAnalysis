{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02823f1d-f7ff-4fe7-8839-a73f141e0387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from os.path import join as joinpath, dirname, basename\n",
    "import pandas as pd\n",
    "from unicodedata import normalize\n",
    "import re\n",
    "import pyspark\n",
    "from os.path import exists as ispath, dirname, basename, join as joinpath, abspath, split as pathsplit, splitext, sep as dirsep, isfile\n",
    "import sys\n",
    "#sys.path.insert(0, dirname(dirname(abspath(__file__))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb003499-49d4-422e-b63a-8cd9c3023908",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/etl_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396dd316-6773-4365-bc6d-a9ce2ad404ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utils/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ebe9e8-fd09-4ff8-9750-32fece24a7cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"source_type\",\"api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58abb6e0-ea1d-4bae-9771-b9ffb848ea2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_type = dbutils.widgets.get(\"source_type\")\n",
    "source_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e35278-2b2a-4e77-acd2-fefac764e55b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_config['src_connector_class_mappings'][source_type]['class_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b69abf-be7a-480c-9dc0-ce9940a3ff50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_config = config.get_config(json_var = connector_config)\n",
    "etl_params = config.get_config(json_var = processing_config)\n",
    "obj_data_ops = data_ops(source_type,code_config)\n",
    "obj_dl = obj_data_ops.get_source_obj()\n",
    "obj_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f876c110-3594-4690-ae05-c8bc2883f368",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GetData(Idata_ops):\n",
    "    valid_fields = ['ward name', 'city', 'api'] #all col names should be in lower\n",
    "    def __init__(self, medium = None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        return None\n",
    "    \n",
    "    def from_file(self, file_path, *args, **kwargs):\n",
    "        format = basename(file_path).split('.')[1]\n",
    "        return spark.read.format(format).option(\"header\", \"true\").load(file_path)\n",
    "    \n",
    "    def check_input(self, attr, do_raise = True):\n",
    "        if not hasattr(self, attr):\n",
    "            if do_raise:\n",
    "                raise ValueError(f'Attribute name is wrong or value has not provided for: {attr}')\n",
    "            return None\n",
    "        return getattr(self, attr, None)\n",
    "    \n",
    "    def normalize_fields(self, fields, replace = None):\n",
    "        out = []\n",
    "        for field in fields:\n",
    "            norm = ' '.join(field.split()) #remove multiple spaces\n",
    "            norm = normalize('NFKD', field).encode('ASCII', 'ignore').decode('utf-8') #if header is in devnagri etc\n",
    "            norm = re.sub(r'[^a-zA-Z0-9\\s]', '', field) #special chars and numbers\n",
    "            norm = norm.replace('/','').replace(':','') \n",
    "            out.append(norm.lower())\n",
    "        return out\n",
    "        \n",
    "    def format_df(self, df, cols_map):\n",
    "        out = {}\n",
    "        if not GetData.valid_fields:\n",
    "            raise ValueError(f'Please provide and field names Getdata.valid_fields = [<valid_column_names>]')\n",
    "        for k, v in cols_map.items():\n",
    "            if k in GetData.valid_fields:\n",
    "                out[k] = dict(df[v])\n",
    "        \n",
    "        return pd.DataFrame(out) if out else {}\n",
    "    \n",
    "    @classmethod\n",
    "    def mergs_dfs(cls, directory_path):\n",
    "        spark_df = spark.read.option(\"delimiter\", \";\").csv(directory_path, header=True, inferSchema=True)\n",
    "\n",
    "        return spark_df\n",
    "        \n",
    "    def read_data(self, api, key = None): #spark DF\n",
    "        try: \n",
    "            out = json.loads(subprocess.run(api, check=True, capture_output=True, text=True, shell = True).stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing cURL command. Return code: {e.returncode}\")\n",
    "            print(\"Error output:\", e.output)\n",
    "            return None\n",
    "        if key:\n",
    "            out = out.get(key, {})\n",
    "        out = pd.json_normalize(out)\n",
    "        print(f'Available columns: {list([i.lower() for i in out.columns])}')\n",
    "        out = self.format_df(out, dict(zip(self.normalize_fields(out.columns), out.columns)))\n",
    "        print(f'Samples: {out.head()}')\n",
    "        return spark.createDataFrame(out)\n",
    "\n",
    "    def write_data(self, df, path):\n",
    "        if isinstance(df, pyspark.sql.dataframe.DataFrame):\n",
    "            df = df.toPandas()\n",
    "        row_count = len(df.index)\n",
    "        if row_count <=0:\n",
    "            print('Got nothing to write in..')\n",
    "            return\n",
    "        df.to_csv(path)\n",
    "        print(f'Data frame saved as csv at {path}')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d327b4f3-49b1-4041-bacc-a73cf2c03fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#sample usesage-\n",
    "\n",
    "#validate fields\n",
    "GetData.valid_fields = ['api', 'description', 'auth', 'https', 'cors', 'link', 'category']\n",
    "\n",
    "#read data\n",
    "api = \"curl --location 'https://api.publicapis.org/entries' \\\n",
    "--header 'Content-Type: application/json'\"\n",
    "d = GetData()\n",
    "df = d.read_data(api, key  = 'entries')\n",
    "\n",
    "#write data\n",
    "bpath = 'api_data.csv'\n",
    "dbutils.fs.mkdirs(bpath)\n",
    "d.write_data(df, bpath)\n",
    "\n",
    "#mergs df\n",
    "#GetData.mergs_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9613e9f-fefc-4cb0-abc8-caf614d18ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8d7ae4b-e5fe-4a89-ada2-5c6aeffe79e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c44bbd-fd74-4322-93d5-044d495b928b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "get_data_from_api",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
