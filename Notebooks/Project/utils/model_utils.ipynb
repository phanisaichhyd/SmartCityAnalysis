{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4085e794-97d0-493d-bbe8-37e9b3293d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531f4427-35f7-4cb4-96bb-aa9d81833769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_data_type(value):\n",
    "    \"\"\"Determines whether a value can be converted to float or not\n",
    "    \n",
    "    Args:\n",
    "        value: a string\n",
    "    Returns:\n",
    "        datatype object float or string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try converting to double\n",
    "        return type(float(value))\n",
    "    except ValueError:\n",
    "        # Return as string if conversion fails\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b27031c-6222-4600-ab4f-63f4b3c04f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_schema(data_frame):\n",
    "    \"\"\"Determines which columns can be converted to float datatype\n",
    "    \n",
    "    Args:\n",
    "        data_frame: a spark dataframe\n",
    "    Returns:\n",
    "        An updated schema for the dataframe with float columns if any\n",
    "    \"\"\"\n",
    "\n",
    "    new_schema = []\n",
    "    for col_name in data_frame.columns:\n",
    "        # Collect unique values in the column\n",
    "        unique_values = data_frame.select(col(col_name)).distinct().collect()\n",
    "\n",
    "        # Infer data type based on unique values\n",
    "        inferred_type = None\n",
    "        for row in unique_values:\n",
    "            value = row[0]\n",
    "            inferred_type = infer_data_type(value)\n",
    "\n",
    "            # Break if a suitable type is found\n",
    "            if inferred_type is not None:\n",
    "                break\n",
    "\n",
    "        # Assign the inferred type or default to StringType\n",
    "        new_type = (\n",
    "            FloatType() if inferred_type == float else\n",
    "            \"string\"\n",
    "        )\n",
    "\n",
    "        # Add the column name and its inferred type to the new schema\n",
    "        new_schema.append((col_name, new_type))\n",
    "\n",
    "    return new_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5ad4bc0-50a7-4277-9592-03484008db0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def nice_case(s: str, sep: str='') -> str:\n",
    "    \"\"\"Converts most strings to NiceCase or Nice{sep}Case.\n",
    "    \n",
    "    Args:\n",
    "        s: Input string to be nice-ified\n",
    "        sep: Separator to be used for nice-ification\n",
    "    \n",
    "    Returns:\n",
    "        Nice-ified string\n",
    "    \"\"\"\n",
    "    \n",
    "    s = re.sub (\"[^A-Za-z0-9]+\", ' ', s)\n",
    "    s = re.sub(r\"( )+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    components = [''.join([word[0].upper(), word[1:]]) for word in s.split(sep=' ')]\n",
    "    \n",
    "    return sep.join(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb7e117-999c-4c7c-988a-1f5e81029ff8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(df, null_threshold_percentage = 0.3):\n",
    "    \"\"\"Preprocesses the dataframe and determines all numerical columns\n",
    "\n",
    "    Args:\n",
    "        df: Spark dataframe to be processed\n",
    "    Returns:\n",
    "        The processed dataframe and list of all numerical columns\n",
    "    \"\"\"\n",
    "    \n",
    "    #Nicefy all column names\n",
    "    for colname in df.columns:\n",
    "        df = df.withColumnRenamed(colname, nice_case(colname))\n",
    "\n",
    "    #Replace 'NA' string with None. This will help in correctly identifying the schema\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(col_name, when(col(col_name) == 'NA', None).otherwise(col(col_name)))\n",
    "\n",
    "    #Dropping columns with null percentage more than the threshold\n",
    "    num_rows = df.count()\n",
    "    null_counts = df.select([(count(when(col(c).isNull(), c))/num_rows).alias(c) for c in df.columns]).collect()[0].asDict() \n",
    "    col_to_drop = [k for k, v in null_counts.items() if v > null_threshold_percentage]\n",
    "    df = df.drop(*col_to_drop)\n",
    "\n",
    "    # Infer the schema\n",
    "    new_schema = infer_schema(df)\n",
    "\n",
    "    #To store column names of all numerical columns\n",
    "    numerical_cols = []\n",
    "\n",
    "    # Apply the new schema to the DataFrame and create a list of numerical columns\n",
    "    for col_name, col_type in new_schema:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(col_type))\n",
    "        if col_type == FloatType():\n",
    "            numerical_cols.append(col_name)\n",
    "\n",
    "    #Fill all null values with 0, since spark ML models cannot us null values\n",
    "    df = df.fillna(0, numerical_cols)\n",
    "\n",
    "    return df, numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f95c3b7-fe28-4d62-a7ef-2f2813120c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def KmeanCluster(df, feature_cols, n_clusters = 3, seed = 1):\n",
    "    \"\"\"Clusters the data points using K-means clustering based on the feature columns\n",
    "\n",
    "        Args:\n",
    "            df: Spark dataframe which contains the features\n",
    "            feature_cols: A list of feature column names\n",
    "            n_clusters: Number of cluster to be created\n",
    "            seed: Seed value to initialise the cluster object\n",
    "        Returns:\n",
    "            A spark dataframe with vectorized features and cluster predictions\n",
    "    \"\"\"    \n",
    "    #Assemble all feature columns into a vector\n",
    "    vecAssembler = VectorAssembler(inputCols = feature_cols, outputCol=\"raw_features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "    #Scale the feature vector\n",
    "    scaler = MinMaxScaler(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "    #Initialize the cluster object\n",
    "    kmeans = KMeans(featuresCol = \"features\", predictionCol=\"Clusters\").setK(n_clusters).setSeed(seed)\n",
    "\n",
    "    #Initialize the pipeline object\n",
    "    pipeline= Pipeline(stages = [vecAssembler, scaler, kmeans])\n",
    "\n",
    "    #Fit the pipeline to the data\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    \n",
    "    #Cluster the data\n",
    "    cluster_df = pipelineModel.transform(df)\n",
    "\n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74396d7c-1220-4dea-af91-27c62ea1f538",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def score_and_pca(df):\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    evaluator.setPredictionCol(\"Clusters\")\n",
    "    silhouette_score = evaluator.evaluate(cluster_df.select(['features', 'Clusters']))\n",
    "\n",
    "    pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "    # Fit the PCA model to the data and transform\n",
    "    pca_model = pca.fit(cluster_df)\n",
    "    pca_result = pca_model.transform(cluster_df)\n",
    "\n",
    "    return silhouette_score, pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42c2cab8-45e6-4e3d-9736-0a1ad532c92f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_clusters_pca_result(pca_result):\n",
    "    pandas_df = pca_result.select(\"pca_features\", \"Clusters\").toPandas()\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "                    pandas_df['pca_features'].apply(lambda x: x[0]),\n",
    "                    pandas_df['pca_features'].apply(lambda x: x[1]),\n",
    "                    c=pandas_df['Clusters'], \n",
    "                    cmap='viridis_r', marker='o', edgecolors='black'\n",
    "                )\n",
    "    \n",
    "    plt.title('K-means Clustering with PCA')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    legend_cluster = plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "model_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
