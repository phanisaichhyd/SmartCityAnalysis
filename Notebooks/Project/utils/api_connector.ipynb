{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9efdc467-a735-4685-bec3-95ecd1caff29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from os.path import join as joinpath, dirname, basename\n",
    "import pandas as pd\n",
    "from unicodedata import normalize\n",
    "import re\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa7e8e2c-56e0-4980-aecf-14e4cd40a160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ../utils/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e0e01a-b85c-4c6b-a359-a52eef745115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GetData(Idata_ops):\n",
    "    valid_fields = ['ward name', 'city', 'api'] #all col names should be in lower\n",
    "    def __init__(self, medium = None, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        return None\n",
    "    \n",
    "    def from_file(self, file_path, *args, **kwargs):\n",
    "        format = basename(file_path).split('.')[1]\n",
    "        return spark.read.format(format).option(\"header\", \"true\").load(file_path)\n",
    "    \n",
    "    def check_input(self, attr, do_raise = True):\n",
    "        if not hasattr(self, attr):\n",
    "            if do_raise:\n",
    "                raise ValueError(f'Attribute name is wrong or value has not provided for: {attr}')\n",
    "            return None\n",
    "        return getattr(self, attr, None)\n",
    "    \n",
    "    def normalize_fields(self, fields, replace = None):\n",
    "        out = []\n",
    "        for field in fields:\n",
    "            norm = ' '.join(field.split()) #remove multiple spaces\n",
    "            norm = normalize('NFKD', field).encode('ASCII', 'ignore').decode('utf-8') #if header is in devnagri etc\n",
    "            norm = re.sub(r'[^a-zA-Z0-9\\s]', '', field) #special chars and numbers\n",
    "            norm = norm.replace('/','').replace(':','') \n",
    "            out.append(norm.lower())\n",
    "        return out\n",
    "        \n",
    "    def format_df(self, df, cols_map):\n",
    "        out = {}\n",
    "        if not GetData.valid_fields:\n",
    "            raise ValueError(f'Please provide and field names Getdata.valid_fields = [<valid_column_names>]')\n",
    "        for k, v in cols_map.items():\n",
    "            if k in GetData.valid_fields:\n",
    "                out[k] = dict(df[v])\n",
    "        \n",
    "        return pd.DataFrame(out) if out else {}\n",
    "    \n",
    "    @classmethod\n",
    "    def mergs_dfs(cls, directory_path):\n",
    "        spark_df = spark.read.option(\"delimiter\", \";\").csv(directory_path, header=True, inferSchema=True)\n",
    "\n",
    "        return spark_df\n",
    "        \n",
    "    def read_data(self, api, key = None): #spark DF\n",
    "        try: \n",
    "            out = json.loads(subprocess.run(api, check=True, capture_output=True, text=True, shell = True).stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing cURL command. Return code: {e.returncode}\")\n",
    "            print(\"Error output:\", e.output)\n",
    "            return None\n",
    "        if key:\n",
    "            out = out.get(key, {})\n",
    "        out = pd.json_normalize(out)\n",
    "        print(f'Available columns: {list([i.lower() for i in out.columns])}')\n",
    "        out = self.format_df(out, dict(zip(self.normalize_fields(out.columns), out.columns)))\n",
    "        print(f'Samples: {out.head()}')\n",
    "        return spark.createDataFrame(out)\n",
    "\n",
    "    def write_data(self, df, path):\n",
    "        if isinstance(df, pyspark.sql.dataframe.DataFrame):\n",
    "            df = df.toPandas()\n",
    "        row_count = len(df.index)\n",
    "        if row_count <=0:\n",
    "            print('Got nothing to write in..')\n",
    "            return\n",
    "        df.to_csv(path)\n",
    "        print(f'Data frame saved as csv at {path}')\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "api_connector",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
