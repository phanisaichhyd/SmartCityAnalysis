{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f05844fa-e986-44d4-94b8-9cce72acc70a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Iutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2429db6-0024-48b4-ae2a-1aadd153593a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from delta.tables import *\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class deltalake(Idata_ops): \n",
    "    '''\n",
    "    Class to perform Data Operations on data lake\n",
    "    '''\n",
    " \n",
    "\n",
    "    @classmethod\n",
    "    def __table_exists(cls,spark,table_name,table_schema) -> bool:\n",
    "        \"\"\"\n",
    "        returns if the table is present in the schema\n",
    "        \"\"\"\n",
    "        spark_table_list = spark.catalog.listTables(table_schema)\n",
    "\n",
    "        return (table_name.lower() in [table.name for table in spark_table_list])\n",
    "    \n",
    "    @classmethod\n",
    "    def __gen_query_cond(cls,src_alias,dest_alias,list_cols) -> str:\n",
    "        \"\"\"\n",
    "        The functions iterate through the list of columns and generate query condition to be used in \n",
    "        merge. Aliases for source and destination are used as passed in paramters\n",
    "        \"\"\"\n",
    "\n",
    "        str_query_cond = \"\"\n",
    "        for cols in list_cols:\n",
    "            str_query_cond = str_query_cond + ' and ' + dest_alias + \".\" + cols + \" = \" + src_alias + \".\" + cols \n",
    "\n",
    "        str_query_cond = str_query_cond[4:]\n",
    "        print(str_query_cond)\n",
    "\n",
    "        return str_query_cond\n",
    "    \n",
    "    @classmethod\n",
    "    def __gen_part_condition(cls,df_data,list_part_cols) -> str:\n",
    "\n",
    "        str_part_cond = \"\"\n",
    "        str_part_value= \"\"\n",
    "\n",
    "        for part_col in list_part_cols:\n",
    "            str_part_value = str(\"'\" + df_data.agg(f.concat_ws(\"','\", f.collect_set(df_data[part_col]))).first()[0])\n",
    "            print(\"Partition Value for part_col\")\n",
    "            print(str_part_value)\n",
    "            if str_part_value != \"\":\n",
    "                str_part_cond =  str_part_cond + ' and existing.' + part_col + ' in (' + str_part_value + \"'\" + ')'\n",
    "\n",
    "        str_part_cond = str_part_cond[4:]\n",
    "\n",
    "        print(\"Partition Condition is \")\n",
    "        print(str_part_cond)\n",
    "        return str_part_cond\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def __create_insert_table(cls,df_data,table_name,table_schema=\"default\",table_format=\"delta\",str_part_cols=\"\"):\n",
    "        \"\"\"\n",
    "        Create Table and insert Data into it using dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        if str_part_cols != None and str(str_part_cols) != \"\":\n",
    "            df_data.write.partitionBy(str_part_cols.split(\",\")).format(table_format).saveAsTable(table_schema + \".\" + table_name)   \n",
    "                \n",
    "        else:\n",
    "             df_data.write.format(table_format).saveAsTable(table_schema + \".\" + table_name)\n",
    "               \n",
    "\n",
    "    @classmethod\n",
    "    def __write_path(cls,df_data,data_path,write_command=\"append\",str_format=\"delta\",str_part_cols=\"\") ->None:\n",
    "        \"\"\"\n",
    "        Inserts Data into the specified location. Supports both append and Overwrite\n",
    "        \"\"\" \n",
    "        row_count = df_data.count()\n",
    "        if row_count <=0:\n",
    "            return\n",
    "        \n",
    "        if str_part_cols != None and str(str_part_cols) != \"\":\n",
    "            df_data.write.partitionBy(str_part_cols.split(\",\")).mode(write_command).format(str_format).save(data_path)\n",
    "        else:\n",
    "            df_data.write.mode(write_command).format(str_format).save(data_path)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def __write_table(cls,spark,insert_command,table_name,df_data,table_schema=\"default\",table_format=\"delta\",str_part_cols=\"\") ->None:\n",
    "        \"\"\"\n",
    "        Inserts Data into the table. Supports both Insert Into and Insert Overwrite\n",
    "        \"\"\"\n",
    "        row_count = df_data.count()\n",
    "        if row_count <=0:\n",
    "            return\n",
    "        \n",
    "        insert_query = \"\"\n",
    "        table_exists = cls.__table_exists(spark = spark,table_name=table_name,table_schema=table_schema)\n",
    "        \n",
    "        if str_part_cols != None and str(str_part_cols) != \"\":\n",
    "            df_data = df_data.select([col for col in df_data.columns if col not in str_part_cols.split(\",\")] + str_part_cols.split(\",\"))\n",
    "        \n",
    "        df_data.createOrReplaceTempView(\"df\")\n",
    "\n",
    "        if table_exists:\n",
    "\n",
    "            print(\"Table Exists. Hence performing \" + insert_command)\n",
    "\n",
    "            insert_query = insert_command + \" table \" + table_schema + \".\" + table_name\n",
    "\n",
    "            if str_part_cols != None and str(str_part_cols) != \"\":                \n",
    "                insert_query = insert_query + \" partition(\" + str_part_cols + \") \"\n",
    "            \n",
    "            insert_query = insert_query + \" select * from df \"\n",
    "            print(insert_query)\n",
    "            spark.sql(insert_query)\n",
    "\n",
    "        else:\n",
    "               print(\"Table doesn't exists. Hence Creating the table and inserting Data\")\n",
    "               cls.__create_insert_table(df_data,table_name,table_schema,table_format,str_part_cols)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def __merge_data(cls,spark,str_merge_cols,delta_table,df_data,table_schema=\"default\",table_format=\"delta\",str_part_cols=\"\",is_table=True) ->None:\n",
    "        \"\"\"\n",
    "            Merge Data into the existing Table Data table_name\n",
    "        \"\"\"\n",
    "        merge_condition = \"\"\n",
    "        str_part_cond = \"\"\n",
    "        table_exists = True\n",
    "\n",
    "        row_count = df_data.count()\n",
    "        if row_count <=0:\n",
    "            return\n",
    "\n",
    "        if str_merge_cols == None or str_merge_cols == \"\":\n",
    "            return\n",
    "\n",
    "        if is_table :\n",
    "            table_exists = cls.__table_exists(spark = spark,table_name=delta_table,table_schema=table_schema)\n",
    "        else:\n",
    "            table_exists = DeltaTable.isDeltaTable(spark, delta_table)\n",
    "        \n",
    "        if table_exists:\n",
    "            print(\"Table Exists. Hence Merging the Data(upsert)\")\n",
    "\n",
    "            merge_condition = cls.__gen_query_cond(src_alias=\"updates\",dest_alias=\"existing\",list_cols=str_merge_cols.split(\",\"))\n",
    "\n",
    "            if str_part_cols != None and str(str_part_cols) != \"\":\n",
    "                str_part_cond = cls.__gen_part_condition(df_data,str_part_cols.split(\",\"))\n",
    "            \n",
    "            merge_condition = merge_condition + \" and \" + str_part_cond\n",
    "\n",
    "            \n",
    "            if is_table:\n",
    "                table = table_schema + \".\" + delta_table\n",
    "                dt = DeltaTable.forName(spark,table)\n",
    "            else:\n",
    "                dt = DeltaTable.forPath(spark,delta_table)\n",
    "\n",
    "            dt.alias(\"existing\").merge(df_data.alias(\"updates\"),merge_condition).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "            \n",
    "        else:\n",
    "            print(\"Table doesn't exists. Hence Creating the table and inserting Data\")\n",
    "            if is_table:\n",
    "                cls.__create_insert_table(df_data=df_data,table_name=delta_table,table_schema=table_schema,\n",
    "                                               table_format=table_format,str_part_cols=str_part_cols)\n",
    "            else:\n",
    "                cls.__write_path(df_data=df_data,data_path=delta_table,write_command=\"overwrite\",\n",
    "                              str_format=table_format,str_part_cols=str_part_cols)\n",
    "        \n",
    "\n",
    "    \n",
    "    def write_data(self,df_data,spark=spark,table_name=\"\",data_path=\"\",table_schema=\"default\",str_format=\"delta\",\n",
    "                            write_command=\"insert into \",str_merge_cols=\"\",str_part_cols=\"\",is_table=True) -> None:\n",
    "        \"\"\"\n",
    "        Writes Data of Dataframe to either Table or a path\n",
    "        \"\"\"\n",
    "        if is_table:\n",
    "            if write_command.lower().__contains__(\"insert\"):\n",
    "                self.__write_table(spark=spark,insert_command=write_command,table_name=table_name,df_data=df_data,\n",
    "                                   table_schema=table_schema,table_format=str_format,str_part_cols=str_part_cols)\n",
    "            else:\n",
    "                print(\"Merge Command\")\n",
    "                self.__merge_data(spark=spark,str_merge_cols=str_merge_cols,delta_table=table_name,df_data=df_data,\n",
    "                                  table_schema=table_schema,table_format=str_format,str_part_cols=str_part_cols,is_table=is_table) \n",
    "        else:\n",
    "            if write_command.lower().__contains__(\"insert\"):\n",
    "                self.__write_path(df_data=df_data,data_path=data_path,write_command=write_command,\n",
    "                              str_format=str_format,str_part_cols=str_part_cols)\n",
    "            else:\n",
    "                print(\"Merge Command\")\n",
    "                self.__merge_data(spark=spark,str_merge_cols=str_merge_cols,delta_table=data_path,df_data=df_data,\n",
    "                                  table_schema=table_schema,table_format=str_format,str_part_cols=str_part_cols,is_table=is_table) \n",
    "    \n",
    "    def read_data(self,spark=spark,str_format=\"delta\",source_type=\"table\",table_name=\"\",query=\"\",table_schema=\"default\",path=\"\",src_options={}) -> DataFrame:\n",
    "        \n",
    "        if source_type.lower() == \"table\":\n",
    "            print(\"Reading Data from SQL\")\n",
    "            if query.strip() != \"\":\n",
    "                print(\"Reading data from Query\")\n",
    "                df_data = spark.sql(query)\n",
    "            else:\n",
    "                print('Reading data from Table')\n",
    "                df_data = spark.sql(\"select * from \" + table_schema + \".\" + table_name )        \n",
    "        else:\n",
    "            df_data = self.__read_data_others(spark,str_format,path=path,src_options=src_options)\n",
    "\n",
    "        return df_data\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def __read_data_others(cls,spark,str_format=\"delta\",path=\"\",src_options={}) -> DataFrame:\n",
    "        df = spark.read.format(str_format).options(**src_options).load(path)\n",
    "        return df\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "deltalake_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
